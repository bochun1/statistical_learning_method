# 监督学习

回归：输入变量与输出变量均为连续变量的预测问题

分类：输出变量为有限个离散变量的预测问题

标注：输入变量与输出变量均为变量序列的预测问题



监督学习模型：概率模型和非概率模型

- 概率模型：条件概率分布 $\text{P(Y|X)}$
- 非概率模型：决策函数 $\text{Y=f(X)}$



统计学习方法三要素：模型、策略、算法

------



## 模型

模型的假设空间 $\mathcal{F}$ 包含所有可能的条件概率分布或决策函数，定义为
$$
\mathcal{F}=\{ f|Y=f_{\theta}(X),\theta\in \text{R}^n\}
\\
\mathcal{F}=\{ P|P_{\theta}(Y|X),\theta\in \text{R}^n\}
$$
其中，$x\in \mathcal{X}$，$y\in \mathcal{Y}$ 参数向量 $\theta$ 取值于 $n$ 维欧式空间 $\text{R}^n$，称为参数空间

------



## 策略

按照什么准则学习或选择最优的模型

- 损失函数（loss function）：度量模型一次预测的好坏
- 风险函数（risk function）：度量模型平均预测的好坏



### 损失函数（loss function）

常用的损失函数

1. 0-1损失函数（0-1 loss function）
   $$
   L(Y,f(X))=
   \left\{\begin{matrix}
   1,\quad Y\neq f(X)
   \\ 
   0,\quad Y=f(X)
   \end{matrix}\right.
   $$

2. 平均损失函数（quadratic loss function）
   $$
   L(Y,f(X))=(Y-f(X))^2
   $$

3. 绝对损失函数（absolute loss function）
   $$
   L(Y,f(X))=|Y-f(X)|
   $$

4. 对数损失函数（logarithmic loss function）
   $$
   L(Y,P(Y|X))=-\log P(Y|X)
   $$



### 风险函数（risk function）

风险函数又称为期望损失（expected loss），由于 $\text{(X,Y)}$ 是随机变量，遵循联合分布 $\text{P(X,Y)}$，故
$$
R_{exp}(f)=E_p[L(Y,f(X))]=\int_{\mathcal{X\times Y}}L(y,f(x))P(x,y)\text{d}x\text{d}y
$$
$R_{exp}$ 是关于联合分布的期望损失，由于 $\text{P(X,Y)}$ 未知，所以 $R_{exp}$ 不能直接计算。根据大数定律，当样本容量趋于无穷时，可以通过样本集（训练集）的平均损失估计期望损失。

定义训练集的平均损失为经验损失 $R_{emp}$ ，则
$$
R_{emp}(f)=\frac{1}{N}\sum_{i=1}^{N}L(y_i,f(x_i))
$$


### 优化问题

经验风险最小化（ERM）：但是，当样本容量有限时，使用经验损失估计期望损失往往不理想，问题就变成了最小化经验损失的优化问题。
$$
R_{erm}(f)=\min_{f\in\mathcal{F}}\frac{1}{N}\sum_{i=1}^{N}L(y_i,f(x_i))
$$
结构风险最小化（SRM）：当样本容量很小时，为避免“过拟合”，会在最小化经验损失后加上表示模型复杂度的正则项或惩罚项。
$$
R_{srm}(f)=\min_{f\in\mathcal{F}}\frac{1}{N}\sum_{i=1}^{N}L(y_i,f(x_i))+\lambda J(f)
$$
其中，$\lambda$为超参数， $J(f)$ 为模型的复杂度，模型越复杂，$J(f)$ 越大。结构风险小需要经验风险与模型复杂度同 时小。

------



## 算法

使用什么计算方法求解优化问题

------



## 模型评估

- 训练误差
  $$
  R_{emp}(\hat{f})=\frac{1}{N}\sum_{i=1}^{N}L(y_i,\hat{f}(x_i))
  $$

- 测试误差
  $$
  e_{test}=\frac{1}{N'}\sum_{i=1}^{N'}L(y_i,\hat{f}(x_i))
  $$

### 加入正则项（惩罚项）

当模型复杂度过大时，过拟合现象就会发生，这时需要加入正则项。
$$
R_{srm}(f)=\min_{f\in\mathcal{F}}\frac{1}{N}\sum_{i=1}^{N}L(y_i,f(x_i))+\lambda J(f)
$$
$\lambda$ 的调整方法——控制变量，网格搜寻

------



## 生成模型与判别模型

生成模型：朴素贝叶斯法、隐马尔可夫模型

特点：可以还原联合概率分布；样本容量增加时，收敛速度更快；存在隐变量时依然可以学习



判别模型：k-近邻法、感知机、决策树、逻辑回归模型、最大熵模型、支持向量机、提升方法、条件随机场

特点：直接学习条件概率和决策函数，准确率高

------



## 感知机

[参考第三章第一节-感知机模型](https://www.bilibili.com/video/av455911968)

线性二分类模型，利用**梯度下降法**对损失函数进行极小化
$$
f(x)=\text{sign}(w\cdot x+b)
$$
几何意义：对于超平面（$n-1$ 维）
$$
w\cdot x+b=0
$$
$w$ 是超平面的法向量，$b$ 是超平面的截距



### 损失函数——误分类点到超平面的总距离

点到超平面的距离
$$
d=\frac{|w\cdot x+b|}{\|w\|}
$$
对于所有误分类点，都有
$$
y=
\left\{\begin{matrix}
+1\quad w\cdot x+b<0
\\
-1\quad w\cdot x+b>0
\end{matrix}\right.
$$
所以
$$
d=\frac{|w\cdot x+b|}{\|w\|}=\frac{-y(w\cdot x+b)}{\|w\|}>0
$$
所有误分类点到超平面的总距离
$$
D=\sum d=-\frac{1}{\|w\|}\sum y(w\cdot x+b)
$$
其中，$\frac{1}{\|w\|}$ 为常数不作考虑。损失函数形式为
$$
L(w,b)=-\sum y(w\cdot x+b)
$$


### 最优化问题求解

原始形式
$$
\min_{w,b}L(w,b)=-\sum_{i=1} y_i(w\cdot x_i+b)
$$
对 $w$，$b$ 求偏导得到梯度，
$$
\nabla_wL(w,b)=\frac{\text{d}L(w,b)}{\text{d}w}=-\sum_{i=1} y_ix_i
\\
\nabla_bL(w,b)=\frac{\text{d}L(w,b)}{\text{d}b}=-\sum_{i=1} y_i
$$
令学习率 $\alpha>0$，$w$，$b$ 迭代公式：
$$
w:=w-\alpha\nabla_wL(w,b)
\\
b:=b-\alpha\nabla_bL(w,b)
$$


对偶形式



#### 收敛性

在训练集上的误分类次数 $k$ 满足不等式
$$
k\leq(\frac{R}{\gamma})^2
$$

------



## K-近邻模型

### 距离度量

$L_p$ 距离：$L_p(x_i,x_j)=(\sum|x_i^{(l)}-x_j^{(l)}|^p)^{\frac{1}{p}}$

$L_p=1$，曼哈顿距离；$L_p=2$，欧氏距离；

特别地，$L_p=\infty$ 时，$L_{\infty}=\max|x_i^{(l)}-x_j^{(l)}|$



### k值的选择

较小的k值：训练误差减小，测试误差增大，模型变复杂，容易过拟合

较大的k值：训练误差增大，测试误差减小，模型变简单



### kd树

为提升k-近邻法搜素效率出现的树形数据结构（二叉树）

------



## 朴素贝叶斯分类

基本假设：条件独立性（$X,Y$ 独立），则
$$
P(X=x|Y=c_k)=P(X^{(1)}=x^{(1)},\dots,X^{(n)}=x^{(n)}|Y=c_k)=\prod_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k)
$$

### 贝叶斯定理

$$
P(Y|X)=\frac{P(X,Y)}{P(X)}=\frac{P(Y)P(X|Y)}{\sum_YP(Y)P(X|Y)}
$$



朴素贝叶斯分类预测为后验概率最大的类，即
$$
y=\arg\max_{c_k}P(Y=c_k)\prod_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k)
$$


### 贝叶斯估计

概率估计可以使用极大似然估计或贝叶斯估计

用极大似然估计可能会出现概率值为0的情况，此时采用贝叶斯估计，

其中，$\lambda\geqslant0$ ，$\lambda=0$ 时为极大似然估计， $\lambda=1$ 时为拉普拉斯平滑，$S_j=|x^{(j)}|$ ，$K$ 为类别数

条件概率的贝叶斯估计形式如下：
$$
P_{\lambda}(X^{(j)}=a_{jl}|Y=c_k)=\frac{\sum_{i=1}^NI(x_i^{(j)}=a_{jl},y_i=c_k)+\lambda}{\sum_{i=1}^NI(y_i=c_k)+S_j\lambda}
$$
先验概率的贝叶斯估计
$$
P_{\lambda}(Y=c_k)=\frac{\sum_{i=1}^NI(y_i=c_k)}{N+K\lambda}
$$

------



## 逻辑回归模型

### [logistic分布](https://mathworld.wolfram.com/LogisticDistribution.html)

分布函数
$$
P(X\leqslant x)=F(x)=\frac{1}{1+e^{-(x-\mu)/\gamma}}
$$
密度函数
$$
f(x)=\frac{e^{-(x-\mu)/\gamma}}{\gamma(1+e^{-(x-\mu)/\gamma})^2}
$$
其中 $\mu$ 是位置参数，$\gamma$ 为尺度参数，分布函数曲线关于 $(\mu,\gamma/2)$ 中心对称

期望：$\mu$，方差：$\frac{1}{3}(\pi\gamma)^2$



单位阶跃函数
$$
y=
\left\{\begin{matrix}
0,\quad z<0
\\
0.5,\quad z=0
\\
1,\quad z>0
\end{matrix}\right.
$$
因为单位阶跃函数不连续，使用logistic分布函数代替，则令 $\mu=0,\gamma=1$，得
$$
y=\frac{1}{1+e^{-x}}
$$
将线性模型 $w\cdot x+b$ 代入得，
$$
y=\frac{1}{1+e^{-(wx+b)}}
$$
此时 $y$ 为事件发生的概率，$1-y$ 为事件未发生的概率，发生的概率/未发生的概率比值
$$
\frac{y}{1-y}=e^{wx+b}
\\
\ln\frac{y}{1-y}=wx+b
$$
该比率反映了一个线性函数的值与概率的关系：

- 线性函数的值越接近 $+\infty$，概率越接近1；
- 线性函数的值越接近 $-\infty$，概率越接近0；

------



## 拉格朗日乘数法

给定
$$
L(x,y,\lambda)=f(x,y)+\lambda\phi(x,y)
$$
$f(x,y)$，$\phi(x,y)$ 在某领域有连续一阶偏导，若存在极值点，则极值点满足
$$
\left\{\begin{matrix}
\frac{\text{d}L}{dx}=0
\\
\frac{\text{d}L}{dy}=0
\\
\frac{\text{d}L}{d\lambda}=0
\end{matrix}\right.
$$
$L(x,y,\lambda)=f(x,y)+\lambda\phi(x,y)$ 称为拉格朗日函数



### 原始问题

给定约束最优化问题
$$
{
\min\quad f(x)
\\
\mathrm{s.t.}\quad{
c_i(x)\leqslant0,\quad i=1,2,\dots,k
\\
h_i(x)=0,\quad j=1,2,\dots,l
}
}
$$
广义朗格朗日函数形式
$$
L(x,\alpha,\beta)=f(x)+\sum_{i=1}^k\alpha_ic_i(x)+\sum_{j=1}^l\beta_jh_j(x)
$$
拉格朗日函数的极大化
$$
\theta_P(x)=\max_{\alpha,\beta:\alpha_i\geq0}L(x,\alpha,\beta)
$$
若 $x$ 满足约束条件，因为 $\sum_{i=1}^k\alpha_ic_i(x)\leq0$，$\sum_{j=1}^l\beta_jh_j(x)=0$。所以 $\theta_P(x)$ 等价于 $f(x)$

所以原始问题（拉格朗日函数的极小极大问题）
$$
\min\theta_P(x)=\min_x\max_{\alpha,\beta:\alpha_i\geqslant0}L(x,\alpha,\beta)
$$


### 对偶问题

定义
$$
\theta_D(\alpha,\beta)=\min_{x}L(x,\alpha,\beta)
$$
原始问题的对偶问题
$$
{
\max_{\alpha,\beta:\alpha\geqslant0}\theta_D(\alpha,\beta)=\max_{\alpha,\beta:\alpha\geqslant0}\min_{x}L(x,\alpha,\beta)
\\
\mathrm{s.t.}\quad\alpha_i\geq0,\quad i=1,2,\dots,k
}
$$


原始问题与对偶问题的关系
$$
\max_{\alpha,\beta:\alpha\geqslant0}\min_{x}L(x,\alpha,\beta)\leq\min_x\max_{\alpha,\beta:\alpha_i\geqslant0}L(x,\alpha,\beta)
$$

------



## 支持向量机

支持向量机的学习策略：**间隔最大化**。

支持向量机包括：

- 当训练集线性可分时，通过硬间隔最大化学习，为线性可分支持向量机
- 当训练集近似线性时，通过软间隔最大化学习，为线性支持向量机
- 当训练集线性不可分时，通过核函数和软间隔最大化学习，为非线性支持向量机



### 线性可分支持向量机

一般地，当训练数据集线性可分时，存在无穷个分离超平面可将两类数据正确分开。**感知机**利用**误分类最小**的策略，求得分离超平面，不过这时的解有无穷多个。**线性可分支持向量机**利用**间隔最大化**求最优分离超平面，这时，解是唯一的。



### 线性支持向量机

线性不可分意味着某些样本点不能满足函数间隔大于等于1的约束条件，引入松弛变量$\xi\geq0$，使得
$$
y_i(wx_i+b)\geq1-\xi_i
$$
目标函数变为
$$
\frac{1}{2}\|w\|^2+C\sum_{i=1}^N\xi_i
$$

------



## EM算法

EM算法是含有隐变量的概率模型参数的极大似然估计法

观测变量 $\text{Y}$，隐变量 $\text{Z}$，联合分布 $\text{P}(\text{Y,Z}|\theta)$，条件分布 $\text{P}(\text{Z}|\text{Y},\theta)$

### 对数似然函数

$$
L(\theta)=\log P(Y|\theta)=\log \sum_ZP(Y,Z|\theta)=\log \sum_ZP(Y|Z,\theta)P(Z|\theta)
$$

### Jensen不等式

对于任意点集 $x\in\mathcal{X}$，若 $\lambda_i\geq0$ 且 $\sum_i\lambda_i=1$，则有凸函数 $f(x)$ 满足：
$$
f(\sum_{i=1}^{M}\lambda_ix_i)\leq\sum_{i=1}^M\lambda_if(x_i)
$$
通过迭代逐步近似极大化 $L(\theta)$，新估计值 $\theta^{(i)}$ 能使 $L(\theta)$ 增加，即$L(\theta)-L(\theta^{(i)})>0$，并逐步达到极大值。
$$
L(\theta)-L(\theta^{(i)})
{=\log(\sum_ZP(Y|Z,\theta)P(Z|\theta))-\log P(Y|\theta^{(i)})
\\
=\log(\sum_ZP(Y|Z,\theta^{(i)})\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Y|Z,\theta^{(i)})})-\log P(Y|\theta^{(i)})
\\
\geqslant\sum_ZP(Z|Y,\theta^{(i)})\log\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Y|Z,\theta^{(i)})}-\log P(Y|\theta^{(i)})
\\
=\sum_ZP(Z|Y,\theta^{(i)})\log\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Y|Z,\theta^{(i)})P(Y|\theta^{(i)})}
}
$$
